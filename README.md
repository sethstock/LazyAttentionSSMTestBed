# LazyAttentionSSMTestBed
Repository to centralize development thrusts on the LazyAttentionSSM (LASSM), which I have designed as a competitor to the Transformer architecture. Right now, it is vastly superior to transformers in windowed and multi-timestep future prediction, seen in the crypto model. It is currently being tested in an LLM and a UNet Image Gen Architecture, and will soon be adapted into a ViLASSM architecture for comparitive testing under the next development thrust.

Still needs to undergo perplexity benchmarking.

License is not included in this GitHub repository. This repository is VIEW ONLY. Download, modification, use, or development of this architecture without express permission and under the direction of Seth Stock is expressly prohibited while the initial research thrusts, validation, and benchmarking are being performed. Only once a paper has been published may this work be used or distributed under a public license. The repository license will be updated when this happens. 
